{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:12<00:00, 13563687.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--CheckPointPath CHECKPOINTPATH]\n",
      "                             [--NumEpochs NUMEPOCHS] [--DivTrain DIVTRAIN]\n",
      "                             [--MiniBatchSize MINIBATCHSIZE]\n",
      "                             [--LoadCheckPoint LOADCHECKPOINT]\n",
      "                             [--LogsPath LOGSPATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\vishw\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-20836Iq3U5c1kvzjD.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "RBE/CS549 Spring 2022: Computer Vision\n",
    "Homework 0: Alohomora: Phase 2 Starter Code\n",
    "\n",
    "Colab file can be found at:\n",
    "    https://colab.research.google.com/drive/1FUByhYCYAfpl8J9VxMQ1DcfITpY8qgsF\n",
    "\n",
    "Author(s): \n",
    "Prof. Nitin J. Sanket (nsanket@wpi.edu), Lening Li (lli4@wpi.edu), Gejji, Vaishnavi Vivek (vgejji@wpi.edu)\n",
    "Robotics Engineering Department,\n",
    "Worcester Polytechnic Institute\n",
    "\n",
    "Code adapted from CMSC733 at the University of Maryland, College Park.\n",
    "\"\"\"\n",
    "\n",
    "# Dependencies:\n",
    "# opencv, do (pip install opencv-python)\n",
    "# skimage, do (apt install python-skimage)\n",
    "# termcolor, do (pip install termcolor)\n",
    "\n",
    "\n",
    "from logging import root\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as tf\n",
    "from torch.optim import AdamW\n",
    "from torchvision.datasets import CIFAR10\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage\n",
    "import PIL\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from skimage import data, exposure\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import argparse\n",
    "import shutil\n",
    "import string\n",
    "from termcolor import colored, cprint\n",
    "import math as m\n",
    "from tqdm.notebook import tqdm\n",
    "# import Misc.ImageUtils as iu\n",
    "from Network.Network import CIFAR10Model, DenseNet, ResNet, VGGNet, ResNeXT\n",
    "from Misc.MiscUtils import *\n",
    "from Misc.DataUtils import *\n",
    "\n",
    "\n",
    "\n",
    "# Don't generate pyc codes\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "    \n",
    "def GenerateBatch(TrainSet, TestSet, TrainLabels, ImageSize, MiniBatchSize):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    TrainSet - Variable with Subfolder paths to train files\n",
    "    NOTE that Train can be replaced by Val/Test for generating batch corresponding to validation (held-out testing in this case)/testing\n",
    "    TrainLabels - Labels corresponding to Train\n",
    "    NOTE that TrainLabels can be replaced by Val/TestLabels for generating batch corresponding to validation (held-out testing in this case)/testing\n",
    "    ImageSize is the Size of the Image\n",
    "    MiniBatchSize is the size of the MiniBatch\n",
    "   \n",
    "    Outputs:\n",
    "    I1Batch - Batch of images\n",
    "    LabelBatch - Batch of one-hot encoded labels \n",
    "    \"\"\"\n",
    "\n",
    "    random_seed = 50\n",
    "    torch.manual_seed(random_seed)\n",
    "    # val_size = 0\n",
    "    # train_size = len(TrainSet) - val_size\n",
    "    # train_ds, val_ds = random_split(TrainSet, [train_size, val_size])\n",
    "    train_loader= DataLoader(TrainSet, MiniBatchSize, shuffle = True)\n",
    "    test_loader = DataLoader(TestSet, MiniBatchSize)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def PrettyPrint(NumEpochs, DivTrain, MiniBatchSize, NumTrainSamples, LatestFile):\n",
    "    \"\"\"\n",
    "    Prints all stats with all arguments\n",
    "    \"\"\"\n",
    "    print('Number of Epochs Training will run for ' + str(NumEpochs))\n",
    "    print('Factor of reduction in training data is ' + str(DivTrain))\n",
    "    print('Mini Batch Size ' + str(MiniBatchSize))\n",
    "    print('Number of Training Images ' + str(NumTrainSamples))\n",
    "    if LatestFile is not None:\n",
    "        print('Loading latest checkpoint with the name ' + LatestFile)              \n",
    "\n",
    "    \n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "\n",
    "def TrainOperation(TrainLabels, NumTrainSamples, ImageSize,\n",
    "                   NumEpochs, MiniBatchSize, SaveCheckPoint, CheckPointPath,\n",
    "                   DivTrain, LatestFile, TrainSet, TestSet, LogsPath, TrainSet1):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    TrainLabels - Labels corresponding to Train/Test\n",
    "    NumTrainSamples - length(Train)\n",
    "    ImageSize - Size of the image\n",
    "    NumEpochs - Number of passes through the Train data\n",
    "    MiniBatchSize is the size of the MiniBatch\n",
    "    SaveCheckPoint - Save checkpoint every SaveCheckPoint iteration in every epoch, checkpoint saved automatically after every epoch\n",
    "    CheckPointPath - Path to save checkpoints/model\n",
    "    DivTrain - Divide the data by this number for Epoch calculation, use if you have a lot of dataor for debugging code\n",
    "    LatestFile - Latest checkpointfile to continue training\n",
    "    TrainSet - The training dataset\n",
    "    LogsPath - Path to save Tensorboard Logs\n",
    "    Outputs:\n",
    "    Saves Trained network in CheckPointPath and Logs to LogsPath\n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    # model = CIFAR10Model(InputSize=3*32*32,OutputSize=10) \n",
    "    # model = VGGNet()\n",
    "    # model = ResNetModel() \n",
    "    model = ResNet(3, 10)\n",
    "    # model = DenseNet()\n",
    "    # model = ResNeXT()\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    Optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
    "\n",
    "    # Tensorboard\n",
    "    # Create a summary to monitor loss tensor\n",
    "\n",
    "    if LatestFile is not None:\n",
    "        CheckPoint = torch.load(CheckPointPath + LatestFile)\n",
    "        # Extract only numbers from the name\n",
    "        StartEpoch = int(''.join(c for c in LatestFile.split('a')[0] if c.isdigit()))\n",
    "        model.load_state_dict(CheckPoint['model_state_dict'])\n",
    "        print('Loaded latest checkpoint with the name ' + LatestFile + '....')\n",
    "    else:\n",
    "        StartEpoch = 0\n",
    "        print('New model initialized....')\n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    train_loader, test_loader = GenerateBatch(TrainSet, TestSet, TrainLabels, ImageSize, MiniBatchSize)\n",
    "    train_loader1, test_loader1 = GenerateBatch(TrainSet1, TestSet, TrainLabels, ImageSize, MiniBatchSize)\n",
    "    for Epochs in tqdm(range(StartEpoch, NumEpochs)):\n",
    "        # train_loader, test_loader = GenerateBatch(TrainSet, TestSet, TrainLabels, ImageSize, MiniBatchSize)\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        PerEpochCounter = 0\n",
    "        for Batch in train_loader:\n",
    "            PerEpochCounter += 1\n",
    "            LossThisBatch = model.training_step(Batch)\n",
    "            train_losses.append(LossThisBatch)\n",
    "            Optimizer.zero_grad()\n",
    "            LossThisBatch.backward()\n",
    "            Optimizer.step()\n",
    "            # Save checkpoint every some SaveCheckPoint's iterations\n",
    "            if PerEpochCounter % SaveCheckPoint == 0:\n",
    "                # Save the Model learnt in this epoch\n",
    "                SaveName =  CheckPointPath + str(Epochs) + 'a' + str(PerEpochCounter) + 'model.ckpt'\n",
    "                \n",
    "                torch.save({'epoch': Epochs,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': Optimizer.state_dict(),\n",
    "                            'loss': LossThisBatch},\n",
    "                             SaveName)\n",
    "                # print('\\n' + SaveName + ' Model Saved...')\n",
    "\n",
    "        # Save model every epoch\n",
    "        print(\"Train loss and accuracy\")\n",
    "        result_train = evaluate(model, train_loader1)\n",
    "        model.epoch_end(Epochs, result_train)\n",
    "        train.append(result_train)\n",
    "        print(\"test loss and accuracy\")\n",
    "        result_test = evaluate(model, test_loader)\n",
    "        model.epoch_end(Epochs, result_test)\n",
    "        test.append(result_test)\n",
    "        SaveName = CheckPointPath + str(Epochs) + 'model.ckpt'\n",
    "        torch.save({'epoch': Epochs,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': Optimizer.state_dict(),\n",
    "                    'loss': LossThisBatch},\n",
    "                     SaveName)\n",
    "        print('\\n' + SaveName + ' Model Saved...')\n",
    "    return train, test\n",
    "\n",
    "def plot_accuracies(train, test):\n",
    "    train_acc = [x['acc'] for x in train]\n",
    "    test_acc = [x['acc'] for x in test]\n",
    "    plt.plot(train_acc, '-x', label = 'TrainSet')\n",
    "    plt.plot(test_acc, '-x', label = 'TestSet')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(ncol=2, loc=\"upper left\")\n",
    "    plt.title('Accuracy vs. No. of epochs')\n",
    "    plt.show()\n",
    "\n",
    "def plot_losses(train, test):\n",
    "    train_acc = [x['loss'] for x in train]\n",
    "    test_acc = [x['loss'] for x in test]\n",
    "    plt.plot(train_acc, '-x', label =  'TrainSet')\n",
    "    plt.plot(test_acc, '-x', label = 'TestSet')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.legend(ncol=2, loc=\"upper right\")\n",
    "    plt.title('Loss vs. No. of epochs')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    None\n",
    "    Outputs:\n",
    "    Runs the Training and testing code based on the Flag\n",
    "    \"\"\"\n",
    "    # Parse Command Line arguments\n",
    "    Parser = argparse.ArgumentParser()\n",
    "    Parser.add_argument('--CheckPointPath', default='../Checkpoints_dense/', help='Path to save Checkpoints, Default: ../Checkpoints/')\n",
    "    Parser.add_argument('--NumEpochs', type=int, default=25, help='Number of Epochs to Train for, Default:50')\n",
    "    Parser.add_argument('--DivTrain', type=int, default=1, help='Factor to reduce Train data by per epoch, Default:1')\n",
    "    Parser.add_argument('--MiniBatchSize', type=int, default=128, help='Size of the MiniBatch to use, Default:1')\n",
    "    Parser.add_argument('--LoadCheckPoint', type=int, default=0, help='Load Model from latest Checkpoint from CheckPointsPath?, Default:0')\n",
    "    Parser.add_argument('--LogsPath', default='LogsRes/', help='Path to save Logs for Tensorboard, Default=Logs/')\n",
    "\n",
    "    transform_train = tf.Compose([tf.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "                                tf.RandomHorizontalFlip(p=0.5),\n",
    "                                tf.ToTensor(),\n",
    "                                tf.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                                ])\n",
    "    transform_test = tf.Compose([tf.ToTensor(),\n",
    "                                tf.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                                ])\n",
    "\n",
    "    TrainSet = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "                                    \n",
    "    TrainSet1 = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=tf.Compose([tf.ToTensor(),\n",
    "                                                                            tf.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                                                                            ]))\n",
    "    TestSet = torchvision.datasets.CIFAR10(root='data/', train=False, transform=transform_test)\n",
    "\n",
    "    Args = Parser.parse_args()\n",
    "    NumEpochs = Args.NumEpochs\n",
    "    DivTrain = float(Args.DivTrain)\n",
    "    MiniBatchSize = Args.MiniBatchSize\n",
    "    LoadCheckPoint = Args.LoadCheckPoint\n",
    "    CheckPointPath = Args.CheckPointPath\n",
    "    LogsPath = Args.LogsPath\n",
    "\n",
    "    # Setup all needed parameters including file reading\n",
    "    SaveCheckPoint, ImageSize, NumTrainSamples, TrainLabels, NumClasses = SetupAll(CheckPointPath)\n",
    "    \n",
    "\n",
    "    # Find Latest Checkpoint File\n",
    "    if LoadCheckPoint==1:\n",
    "        LatestFile = FindLatestModel(CheckPointPath)\n",
    "    else:\n",
    "        LatestFile = None\n",
    "    \n",
    "    # Pretty print stats\n",
    "    PrettyPrint(NumEpochs, DivTrain, MiniBatchSize, NumTrainSamples, LatestFile)\n",
    "\n",
    "    train, test = TrainOperation(TrainLabels, NumTrainSamples, ImageSize,\n",
    "                NumEpochs, MiniBatchSize, SaveCheckPoint, CheckPointPath,\n",
    "                DivTrain, LatestFile, TrainSet, TestSet, LogsPath, TrainSet1)\n",
    "\n",
    "    plot_accuracies(train, test)\n",
    "    plot_losses(train, test)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 289\u001b[0m\n\u001b[0;32m    285\u001b[0m     plot_losses(train, test)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 259\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    253\u001b[0m TrainSet1 \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    254\u001b[0m                                     download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mCompose([tf\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m    255\u001b[0m                                                                         tf\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.4914\u001b[39m, \u001b[38;5;241m0.4822\u001b[39m, \u001b[38;5;241m0.4465\u001b[39m), (\u001b[38;5;241m0.2023\u001b[39m, \u001b[38;5;241m0.1994\u001b[39m, \u001b[38;5;241m0.2010\u001b[39m))\n\u001b[0;32m    256\u001b[0m                                                                         ]))\n\u001b[0;32m    257\u001b[0m TestSet \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform_test)\n\u001b[1;32m--> 259\u001b[0m Args \u001b[38;5;241m=\u001b[39m \u001b[43mParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m NumEpochs \u001b[38;5;241m=\u001b[39m Args\u001b[38;5;241m.\u001b[39mNumEpochs\n\u001b[0;32m    261\u001b[0m DivTrain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(Args\u001b[38;5;241m.\u001b[39mDivTrain)\n",
      "File \u001b[1;32mc:\\Users\\vishw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:1872\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[0;32m   1871\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1872\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\vishw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2628\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m   2629\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vishw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[0;32m   2616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2617\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
